{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:37:11.262431200Z",
     "start_time": "2023-06-15T22:37:09.191221600Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D\n",
    "from keras.optimizers import SGD\n",
    "from numpy.random import uniform\n",
    "import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:18.680435300Z",
     "start_time": "2023-06-15T22:54:18.675438Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_mixed_shape(shape1, shape2, shape3, shape4):\n",
    "    shape = []\n",
    "    shape.extend(shape1[:15].tolist())\n",
    "    shape.extend(shape2[15:30].tolist())\n",
    "    shape.extend(shape3[30:45].tolist())\n",
    "    shape.extend(shape4[45:60].tolist())\n",
    "    return pd.DataFrame([shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:19.190392400Z",
     "start_time": "2023-06-15T22:54:19.181394100Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_noisy_shape(shape):\n",
    "    noisyShape = []\n",
    "    for val in shape:\n",
    "        noisyShape.append(val + uniform(-0.3, 0.3))\n",
    "    return pd.DataFrame([noisyShape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:20.113915900Z",
     "start_time": "2023-06-15T22:54:19.793809400Z"
    }
   },
   "outputs": [],
   "source": [
    "symbol_data = pd.read_csv(\"data/3DMaster_dataset_fixed.csv\", header=None)\n",
    "#symbol_data = pd.read_csv(\"data/dataset1_fixed.csv\", header=None)\n",
    "X = symbol_data.iloc[:, :-1]\n",
    "Y = symbol_data.iloc[:, -1:]\n",
    "Y = Y.replace(\"Time\", 0)\n",
    "Y = Y.replace(\"Air\", 1)\n",
    "Y = Y.replace(\"Fire\", 2)\n",
    "#Y = Y.replace(\"Earth\", 3)\n",
    "#Y = Y.replace(\"Water\", 4)\n",
    "Y = Y.replace(\"Lightning\", 3) #cant have empty entries this must be 3\n",
    "print(Y)\n",
    "print(X)\n",
    "for i in range(500):\n",
    "    random_symbols = X.sample(n=4)\n",
    "    X = pd.concat([X, generate_mixed_shape(random_symbols.iloc[0], random_symbols.iloc[1], random_symbols.iloc[2], random_symbols.iloc[3])])\n",
    "    Y = pd.concat([Y, pd.DataFrame([3], columns=[60])])\n",
    "\n",
    "for i in range(500):\n",
    "    random_symbol = X.sample(n=1)\n",
    "    X = pd.concat([X, pd.DataFrame(generate_noisy_shape(random_symbol.iloc[0]))])\n",
    "    Y = pd.concat([Y, pd.DataFrame([3], columns=[60])])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(X), Y, test_size=0.20)\n",
    "\n",
    "y_test = y_test.values.flatten().astype(\"float64\")\n",
    "y_train = y_train.values.flatten().astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:21.172048800Z",
     "start_time": "2023-06-15T22:54:21.130783900Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_3d = [np.array(x_train[i]).reshape(-1, 3) for i in range(len(x_train))]\n",
    "x_test_3d = [np.array(x_test[i]).reshape(-1, 3) for i in range(len(x_test))]\n",
    "\n",
    "x_train_3d = tf.stack(x_train_3d)\n",
    "x_test_3d = tf.stack(x_test_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:22.488422800Z",
     "start_time": "2023-06-15T22:54:22.480425100Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_test_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:24.698655Z",
     "start_time": "2023-06-15T22:54:24.646220Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "points = x_train_3d[4]\n",
    "clas = y_train[4]\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2])\n",
    "ax.set_axis_off()\n",
    "ax.view_init(elev=90, azim=0, roll=90)\n",
    "plt.show()\n",
    "print(\"label: \", clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:26.529478600Z",
     "start_time": "2023-06-15T22:54:26.517469Z"
    }
   },
   "outputs": [],
   "source": [
    "def augment(points, label):\n",
    "    # jitter points\n",
    "    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n",
    "    # shuffle points\n",
    "    points = tf.random.shuffle(points)\n",
    "    return points, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:27.422455600Z",
     "start_time": "2023-06-15T22:54:27.404454Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_POINTS = 20\n",
    "NUM_CLASSES = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:27.422455600Z",
     "start_time": "2023-06-15T22:54:27.404454Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_3d, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_3d, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(len(x_train_3d)).map(augment).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:27.869390700Z",
     "start_time": "2023-06-15T22:54:27.802913300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:54:28.435759500Z",
     "start_time": "2023-06-15T22:54:28.431763200Z"
    }
   },
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalRegularizerTest(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'num_features': float(self.num_features), 'l2reg': float(self.l2reg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnet(inputs, num_features, subfix):\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    # reg = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01, mode=\"columns\")\n",
    "    # reg = tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n",
    "    reg = OrthogonalRegularizerTest(num_features)\n",
    "\n",
    "    x = conv_bn(inputs, 32, subfix + \"_1\")\n",
    "    x = conv_bn(x, 64, subfix + \"_2\")\n",
    "    x = conv_bn(x, 512, subfix + \"_3\")\n",
    "    x = layers.GlobalMaxPooling1D(name=\"maxpool_\" + subfix)(x)\n",
    "    x = dense_bn(x, 256, subfix + \"_4\")\n",
    "    x = dense_bn(x, 256, subfix + \"_5\")\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "        name=\"T_\" + subfix,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features), name=\"T_reshape_\" + subfix)(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1), name=\"T_dot_\" + subfix)([inputs, feat_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(x, filters, subfix):\n",
    "    res = x\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\", name=\"conv_\" + subfix)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0, name=\"bn_\" + subfix)(x)\n",
    "    if res.type_spec.shape == x.type_spec.shape:\n",
    "        #print(subfix)\n",
    "        x =layers.Add()([x,res])\n",
    "    return layers.Activation(\"relu\", name=\"relu_\" + subfix)(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters, subfix):\n",
    "    res = x\n",
    "    x = layers.Dense(filters, name=\"dense_\" + subfix)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0, name=\"bn_\" + subfix)(x)\n",
    "    if res.type_spec.shape == x.type_spec.shape:\n",
    "        #print(subfix)\n",
    "        x = layers.Add()([x,res])\n",
    "    return layers.Activation(\"relu\", name=\"relu_\" + subfix)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(NUM_POINTS, 3))\n",
    "\n",
    "embed_dim = 3  # Embedding size for each token\n",
    "num_heads = 3  # Number of attention heads\n",
    "ff_dim = 516  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "x = transformer_encoder(inputs, embed_dim, num_heads, ff_dim)\n",
    "x = tnet(x, 3, \"tnet1\")\n",
    "x = conv_bn(x, 32, \"1\")\n",
    "x = conv_bn(x, 32,  \"2\")\n",
    "x = tnet(x, 32, \"tnet2\")\n",
    "x = conv_bn(x, 64, \"3\")\n",
    "x = conv_bn(x, 64, \"4\")\n",
    "x = conv_bn(x, 512, \"5\")\n",
    "x = layers.GlobalMaxPooling1D(name=\"maxpool1\")(x)\n",
    "x = dense_bn(x, 256, \"6\")\n",
    "#x = layers.Dropout(0.3)(x)\n",
    "x = dense_bn(x, 256, \"7\")\n",
    "#x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"outputs\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_test\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model_test\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 7s 214ms/step - loss: 1.3449 - sparse_categorical_accuracy: 0.9406 - val_loss: 1.3271 - val_sparse_categorical_accuracy: 0.9317\n",
      "Epoch 6/500\n",
      "34/35 [============================>.] - ETA: 0s - loss: 1.3002 - sparse_categorical_accuracy: 0.9467\n",
      "Epoch 6: val_sparse_categorical_accuracy did not improve from 0.93165\n",
      "35/35 [==============================] - 1s 24ms/step - loss: 1.2923 - sparse_categorical_accuracy: 0.9469 - val_loss: 1.4895 - val_sparse_categorical_accuracy: 0.9029\n",
      "Epoch 7/500\n",
      " 7/35 [=====>........................] - ETA: 0s - loss: 1.3255 - sparse_categorical_accuracy: 0.9420"
     ]
    }
   ],
   "source": [
    "filepath = \"best_model_test\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(train_dataset, epochs=500, validation_data=test_dataset, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best_model_test', custom_objects={\"OrthogonalRegularizerTest\": OrthogonalRegularizerTest},\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:55:37.296284500Z",
     "start_time": "2023-06-15T22:55:37.078714600Z"
    }
   },
   "outputs": [],
   "source": [
    "# show history plot\n",
    "plt.plot(model.history.history[\"sparse_categorical_accuracy\"])\n",
    "plt.plot(model.history.history[\"val_sparse_categorical_accuracy\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:55:41.459676600Z",
     "start_time": "2023-06-15T22:55:40.652588900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion matrix\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "#cm = [c[0:-1] for c in cm[0:-1]]\n",
    "print(cm)\n",
    "# plot\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title(\"Confusion matrix \")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:55:53.390992300Z",
     "start_time": "2023-06-15T22:55:52.596607400Z"
    }
   },
   "outputs": [],
   "source": [
    "class_map = {}\n",
    "class_map[0] = \"Time\"\n",
    "class_map[1] = \"Wind\"\n",
    "class_map[2] = \"Fire\"\n",
    "class_map[3] = \"Lightning\"\n",
    "class_map[4] = \"Other\"\n",
    "#class_map[5] = \"Lightning\"\n",
    "#class_map[6] = \"Other\"\n",
    "\n",
    "data = train_dataset.take(1)\n",
    "\n",
    "points, labels = list(data)[0]\n",
    "points = points[:8, ...]\n",
    "labels = labels[:8, ...]\n",
    "\n",
    "# run test data through model\n",
    "preds = model.predict(points)\n",
    "preds = tf.math.argmax(preds, -1)\n",
    "\n",
    "points = points.numpy()\n",
    "\n",
    "# plot points with predicted class and label\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(2, 4, i + 1, projection=\"3d\")\n",
    "    ax.scatter(points[i, :, 0], points[i, :, 1], points[i, :, 2])\n",
    "    ax.set_title(\n",
    "        \"pred: {:}, label: {:}\".format(\n",
    "            class_map[preds[i].numpy()], class_map[labels.numpy()[i]]\n",
    "        )\n",
    "    )\n",
    "    ax.set_axis_off()\n",
    "    ax.view_init(elev=90, azim=0, roll=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:56:11.267514400Z",
     "start_time": "2023-06-15T22:56:03.958509100Z"
    }
   },
   "outputs": [],
   "source": [
    "# save with custom objects\n",
    "model.save('model_3dmaster_with_other_transformer')\n",
    "\n",
    "model.save('model_3dmaster_with_other_transformer.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:56:13.197985400Z",
     "start_time": "2023-06-15T22:56:12.986691500Z"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T22:56:58.180600300Z",
     "start_time": "2023-06-15T22:56:51.426109800Z"
    }
   },
   "outputs": [],
   "source": [
    "!python -m tf2onnx.convert --saved-model best_model_test --output model_3dmaster_with_other_transformer.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:55:02.652745600Z",
     "start_time": "2023-06-12T03:55:00.293832Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model('model_3dmaster_with_other_transformer', custom_objects={\"OrthogonalRegularizerTest\": OrthogonalRegularizerTest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
